{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对文献进行提取并且分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import glob\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings,HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取文献"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = glob.glob('./pdf_sources/*.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取pdf为字符文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n",
      "出错了!...\n"
     ]
    }
   ],
   "source": [
    "pdf_pageText = []\n",
    "meta_info = []\n",
    "# 关键词列表\n",
    "pdf_keywords = []\n",
    "# 摘要列表\n",
    "pdf_subject = []\n",
    "allpageText = \"\"\n",
    "\n",
    "for pdf in result :\n",
    "    try :\n",
    "        ipageText = \"\"\n",
    "        pdfdoc = fitz.open(pdf)\n",
    "        for page in pdfdoc:\n",
    "            ipageText += page.get_text(\"text\")\n",
    "        allpageText += ipageText\n",
    "        pdf_pageText.append(ipageText)\n",
    "        # 生成元数据\n",
    "        ipm = pdfdoc.metadata\n",
    "        # 生成关键词和摘要列表\n",
    "        pdf_keywords.append(ipm[\"keywords\"])\n",
    "        pdf_subject.append(ipm[\"subject\"])\n",
    "    except :\n",
    "        print(\"出错了!...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对字符文本进行分块处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = CharacterTextSplitter(\n",
    "#     separator = \"\\n\",\n",
    "#     chunk_size = 1000,\n",
    "#     chunk_overlap = 200,\n",
    "#     length_function=len\n",
    "# )\n",
    "# chunks = text_splitter.split_text(allpageText)\n",
    "\n",
    "chunks = pdf_keywords + pdf_subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "user提出一个quesiton，首先与prompt进行相似度匹配，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 将处理好的分块保留为pageText.json格式的文件\n",
    "pd.DataFrame(chunks).to_json('pageText.json')\n",
    "\n",
    "json_path = \"F:\\\\工作以及比赛\\\\大一统框架\\\\dd\\\\try2\\\\app\\\\backproject\\\\app01\\\\chatapi\\\\pageText.json\"\n",
    "# 读取pageText.json格式的文件\n",
    "chunks = pd.read_json(json_path).iloc[:,0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进行向量相似度分析，给出文本分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# 读取本地模型文件\n",
    "model = INSTRUCTOR(\"F:\\\\工作以及比赛\\\\大一统框架\\\\dd\\\\try2\\\\t1\\\\model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对分块进行编码处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = model.encode(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编码持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将encode保存为chunksEncoding.json文件\n",
    "pd.DataFrame(s1).to_json(\"./chunksEncoding.json\")\n",
    "\n",
    "# 读取chunksEncoding.json文件\n",
    "s1 = np.array(pd.read_json(\"./chunksEncoding.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the core microorganisms in anaerobic digestion?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编码问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = model.encode(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# s1.shape\n",
    "\n",
    "s2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行向量相似化检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 10\n",
    "document = np.array(chunks)[cosine_similarity(s1,s2.reshape(1,-1)).squeeze().argsort()[0:max_len].tolist()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_title = np.array([\"Anaerobic digestion (material addition effects)\",\n",
    "                    \"Microbial effects of anaerobic digestion\",\n",
    "                    \"Analysis of the nature of anaerobic digestion systems\",\n",
    "                    \"Impact of anaerobic digestion on gas production\",])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于两层相似度检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_match(question) :\n",
    "    # 对问题进行编码处理\n",
    "    question_encode = model.encode(question)\n",
    "    # 第一层检索\n",
    "    # 根据每个prompt的title进行优先匹配\n",
    "    prompt_title = np.array([\"Anaerobic digestion (material addition effects)\",\n",
    "                        \"Microbial effects of anaerobic digestion\",\n",
    "                        \"Analysis of the nature of anaerobic digestion systems\",\n",
    "                        \"Impact of anaerobic digestion on gas production\",])\n",
    "    title_encode = model.encode(prompt_title)\n",
    "    one_vector = cosine_similarity(title_encode,question_encode.reshape(1,-1))\n",
    "    # 第二层检索\n",
    "    # 根据每个prompt的content进行二次匹配\n",
    "        # 给出文件列表\n",
    "    dir_list = [f'./t{i}.txt' for i in range(1,5)]\n",
    "    content_list = []\n",
    "        # 读取四种类型的prompt\n",
    "    for dir in dir_list :\n",
    "        with open(dir , 'r' , encoding='utf8') as f :\n",
    "            content = f.read()\n",
    "        content_list.append(content)\n",
    "    content_list = model.encode(np.array(content_list))\n",
    "    two_vector = cosine_similarity(content_list,question_encode.reshape(1,-1))\n",
    "\n",
    "    # print(f\"第一层认为第{one_vector.argmax()+1}个最匹配,第二层认为第{two_vector.argmax()+1}个最匹配,综合认为第{(0.8 * one_vector + 0.2 * two_vector).argmax() + 1}个最匹配\")\n",
    "    return (0.8 * one_vector + 0.2 * two_vector).argmax() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_list = [\n",
    "#     \"Direct electron transfer promotes anaerobic digestion mechanism?\",\n",
    "#     \"How to further improve the efficiency of anaerobic digestion, such as processing the straw load to the rumen.\",\n",
    "#     \"What are the core microorganisms in anaerobic digestion?\",\n",
    "#     \"Does oxygen improve anaerobic digestion efficiency?\",\n",
    "#     \"Is around 40 degrees suitable for efficient anaerobic methane production?\",\n",
    "#     \"Effect of Methanocelleus on anaerobic digestion of food waste? \",\n",
    "#     \"What effect does the addition of biochar have on the biogas production of anaerobic digestion of food waste? What should its optimal addition amount be?\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 读取quesiton_list\n",
    "question_dir_list = [f'./question_list/question_list{i}.txt' for i in range(1,5)]\n",
    "question_list_gpt = []\n",
    "\n",
    "for iquestion_list in question_dir_list :\n",
    "    with open(iquestion_list,'r') as f :\n",
    "        question_list_content = f.read()\n",
    "    question_list_gpt += question_list_content.split('\\n')\n",
    "\n",
    "# 对问题进行去重处理\n",
    "question_list = list(set(question_list_gpt))\n",
    "\n",
    "# 给定一个乱序问题列表\n",
    "question_list_copy = list(question_list)\n",
    "\n",
    "# 打乱顺序\n",
    "np.random.shuffle(question_list_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_list = []\n",
    "for iquestion in question_list_copy :\n",
    "    question_index = question_match(iquestion)\n",
    "    source_index = question_list.index(iquestion) // 100 + 1\n",
    "    show_list.append([iquestion,question_index,source_index,prompt_title[question_index-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(show_list,columns=['question','source document','match document','match title']).to_excel('record.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "db = pd.read_excel('./record.xlsx')\n",
    "v1 = db.iloc[:,2]\n",
    "v2 = db.iloc[:,3]\n",
    "accuracy_score(v1,v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature =np.array([model.encode(iquestion) for iquestion in question_list])\n",
    "label = np.array([question_list_copy.index(iquestion) // 100 + 1 for iquestion in question_list]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_one_hot = []\n",
    "class_type = 4\n",
    "for i in range(label.shape[0]) :\n",
    "    eye = np.zeros([4])\n",
    "    eye[label[i]-1] = 1\n",
    "    label_one_hot.append(eye)\n",
    "label = np.array(label_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 772)"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([feature,label],axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查cuda是否可用\n",
    "torch.cuda.is_available()\n",
    "# 清空缓存\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3344003829.py, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[642], line 31\u001b[1;36m\u001b[0m\n\u001b[1;33m    nn.Dropout(p=drop_prod)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def score_func(batch_size,hidden_size,drop_prod,lr,epochs) :\n",
    "    batch_size = int(batch_size)\n",
    "    hidden_size = int(hidden_size)\n",
    "    epochs = int(epochs)\n",
    "    # 划分训练集\n",
    "    trainset,testset = train_test_split(np.concatenate([feature,label],axis=1),test_size=0.3,shuffle=True)\n",
    "\n",
    "    # 定义特征以及标签\n",
    "    feature_train , label_train = trainset[:,0:trainset.shape[-1]-4] , trainset[:,trainset.shape[-1]-4:trainset.shape[-1]]\n",
    "    feature_test , label_test = testset[:,0:testset.shape[-1]-4] , testset[:,testset.shape[-1]-4:testset.shape[-1]]\n",
    "\n",
    "    # 定义数据集\n",
    "    traindataset = torch.utils.data.TensorDataset(torch.Tensor(feature_train),torch.Tensor(label_train))\n",
    "    testdataset = torch.utils.data.TensorDataset(torch.Tensor(feature_test),torch.Tensor(label_test))\n",
    "\n",
    "    # 定义加载器\n",
    "    # batch_size = 10\n",
    "    traindataloader = torch.utils.data.DataLoader(traindataset,shuffle=True,batch_size=batch_size,drop_last=True)\n",
    "    testdataloader = torch.utils.data.DataLoader(testdataset,shuffle=True,batch_size=batch_size,drop_last=True)\n",
    "\n",
    "    # 定义神经网络结构\n",
    "    class network(nn.Module) :\n",
    "        def __init__(self,input_size,hidden_size,drop_prod) :\n",
    "            super().__init__()\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(in_features = input_size,out_features = hidden_size,bias = True)\n",
    "                nn.ReLU()\n",
    "                nn.Dropout(p=drop_prod)\n",
    "                nn.Linear(in_features = hidden_size,out_features = 2 * hidden_size,bias = True)\n",
    "                nn.ReLU()\n",
    "                nn.Dropout(p=drop_prod)\n",
    "                nn.Linear(in_features = 2 * hidden_size,out_features = class_type,bias = True)\n",
    "                nn.Dropout(p=drop_prod)\n",
    "            )\n",
    "        def forward(self,x) :\n",
    "            return self.model(x)\n",
    "\n",
    "    # 实例化网络\n",
    "    input_size = feature.shape[-1]\n",
    "    # hidden_size = 256\n",
    "    # drop_prod = 0.8\n",
    "    net = network(input_size=input_size,hidden_size=hidden_size,drop_prod=drop_prod)\n",
    "\n",
    "    # 迁移模型\n",
    "    # net.to(\"cuda:0\")\n",
    "\n",
    "    # 定义优化器\n",
    "    # lr = 1e-3\n",
    "    optimizer =torch.optim.Adam(net.parameters(),lr)\n",
    "\n",
    "    # 定义损失函数\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 初始化参数\n",
    "    for name,param in net.named_parameters() :\n",
    "        if \"bias\" in name :\n",
    "            nn.init.constant_(param,val=0)\n",
    "        elif \"weight\" in name :\n",
    "            nn.init.normal_(param,mean=0,std=0.01)\n",
    "\n",
    "    # epochs = 3000\n",
    "    loss_list = []\n",
    "    for epoch in range(epochs) :\n",
    "        ls = 0\n",
    "        lt = 0\n",
    "        a = 0\n",
    "        for pi_X,pi_y in traindataloader :\n",
    "            # 迁移数据\n",
    "            # pi_X = pi_X.to(\"cuda:0\")\n",
    "            # pi_y = pi_y.to(\"cuda:0\")\n",
    "            l = loss(net(pi_X),pi_y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            ls += l\n",
    "            with torch.no_grad():\n",
    "                #开启评估模式\n",
    "                net.eval()\n",
    "                for pi_X_test,pi_y_test in testdataloader :\n",
    "                    # pi_X_test = pi_X_test.to(\"cuda:0\")\n",
    "                    # pi_y_test = pi_y_test.to(\"cuda:0\")\n",
    "                    lt += loss(net(pi_X_test),pi_y_test)\n",
    "                    a += ((net(pi_X_test).argmax(dim=1) == pi_y_test.argmax(dim=1))**2).sum()\n",
    "            net.train()\n",
    "    return -a , net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_bayes = (10,100)\n",
    "hidden_size_bayes = (100,400)\n",
    "drop_prod_bayes = (0.2,0.8)\n",
    "lr_bayes = (1e-3,5e-2)\n",
    "epochs_bayes = (1000,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "score_func_score = lambda **kwargs : score_func(**kwargs)[0]\n",
    "\n",
    "param_dict = {\n",
    "    \"batch_size\":batch_size_bayes,\n",
    "    \"hidden_size\":hidden_size_bayes,\n",
    "    \"drop_prod\":drop_prod_bayes,\n",
    "    \"lr\":lr_bayes,\n",
    "    \"epochs\":epochs_bayes,\n",
    "}\n",
    "# opt = BayesianOptimization(score_func,param_dict)\n",
    "opt = BayesianOptimization(score_func_score,param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.maximize(n_iter=15, init_points=3)\n",
    "print(opt.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "network(\n",
       "  (linear1): Linear(in_features=768, out_features=248, bias=True)\n",
       "  (activefunc1): ReLU()\n",
       "  (drop1): Dropout(p=0.779723130418032, inplace=False)\n",
       "  (linear2): Linear(in_features=248, out_features=4, bias=True)\n",
       "  (drop2): Dropout(p=0.779723130418032, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用最优化参数进行训练并且保存模型\n",
    "_,mynet = score_func(**opt.max[\"params\"])\n",
    "mynet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226],\n",
       "        [-0.0155,  0.0232, -0.0615, -0.0226]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynet.eval()\n",
    "mynet(torch.Tensor(feature_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 0, 3, 3, 2, 2, 1, 3, 0, 3, 1, 0, 1, 1, 0, 1, 2, 2, 1, 1,\n",
       "       0, 1, 1, 3, 2, 2, 0, 0, 2, 2, 1, 1, 1, 0, 0, 0, 3, 2, 0, 2, 1, 0,\n",
       "       3, 2, 0, 1, 0, 2, 1, 3, 3, 1, 1, 1, 2, 3, 3, 3, 3, 2, 2, 1, 2, 0,\n",
       "       2, 0, 2, 3, 2, 2, 1, 1, 1, 3, 0, 1, 1, 0, 3, 1, 2, 2, 1, 1, 2, 2,\n",
       "       2, 0, 0, 3, 3, 1, 0, 0, 0, 3, 3, 2, 0, 0, 2, 3, 3, 1, 2, 2, 2, 0,\n",
       "       0, 3, 1, 1, 1, 0, 3, 0, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynet(torch.Tensor(feature_test)).argmax(dim=1)\n",
    "\n",
    "label_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_37508\\3827524707.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  nn.Softmax()(mynet(torch.Tensor(np.ones([10,1]).dot(test_question_encode.reshape(1,-1))))).argmax(dim=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynet.eval()\n",
    "test_question = \"Name some common Methanogenic bacteria.\"\n",
    "test_question_encode = model.encode(test_question)\n",
    "nn.Softmax()(mynet(torch.Tensor(np.ones([10,1]).dot(test_question_encode.reshape(1,-1))))).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 768])"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature_test.shape\n",
    "\n",
    "# mynet(feature_test)\n",
    "\n",
    "# for pi_X,pi_y in traindataloader :\n",
    "#     print(pi_X.shape)\n",
    "# mynet(pi_X)\n",
    "\n",
    "mynet(torch.Tensor(feature_test))\n",
    "torch.Tensor(feature_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mynet(torch.Tensor(test_question_encode.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先根据question根据prompt进行分类\n",
    "\n",
    "# 其次对参考文献做分类问题，算法为二次相似度检验算法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_match(question) :\n",
    "    # 对问题进行编码处理\n",
    "    question_encode = model.encode(question)\n",
    "    # 第一层检索\n",
    "    # 根据每个prompt的title进行优先匹配\n",
    "    prompt_title = np.array([\"Anaerobic digestion (material addition effects)\",\n",
    "                        \"Microbial effects of anaerobic digestion\",\n",
    "                        \"Analysis of the nature of anaerobic digestion systems\",\n",
    "                        \"Impact of anaerobic digestion on gas production\",])\n",
    "    title_encode = model.encode(prompt_title)\n",
    "    one_vector = cosine_similarity(title_encode,question_encode.reshape(1,-1))\n",
    "    # 第二层检索\n",
    "    # 根据每个prompt的content进行二次匹配\n",
    "        # 给出文件列表\n",
    "    dir_list = [f'./t{i}.txt' for i in range(1,5)]\n",
    "    content_list = []\n",
    "        # 读取四种类型的prompt\n",
    "    for dir in dir_list :\n",
    "        with open(dir , 'r' , encoding='utf8') as f :\n",
    "            content = f.read()\n",
    "        content_list.append(content)\n",
    "    content_list = model.encode(np.array(content_list))\n",
    "    two_vector = cosine_similarity(content_list,question_encode.reshape(1,-1))\n",
    "\n",
    "    # print(f\"第一层认为第{one_vector.argmax()+1}个最匹配,第二层认为第{two_vector.argmax()+1}个最匹配,综合认为第{(0.8 * one_vector + 0.2 * two_vector).argmax() + 1}个最匹配\")\n",
    "    return (0.8 * one_vector + 0.2 * two_vector).argmax() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章关键词列表\n",
    "pdf_keywords\n",
    "# 文章摘要列表\n",
    "pdf_subject\n",
    "\n",
    "# 第一层检索\n",
    "# 对pdf标题进行编码处理\n",
    "pdf_keywords_encode = model.encode(pdf_keywords)\n",
    "\n",
    "# 对prompt标题进行编码处理\n",
    "prompt_title = np.array([\"Anaerobic digestion (material addition effects)\",\n",
    "                    \"Microbial effects of anaerobic digestion\",\n",
    "                    \"Analysis of the nature of anaerobic digestion systems\",\n",
    "                    \"Impact of anaerobic digestion on gas production\",])\n",
    "title_encode = model.encode(prompt_title)\n",
    "one_vector_list = []\n",
    "\n",
    "for i in range(pdf_keywords_encode.shape[0]) :\n",
    "    one_vector = cosine_similarity(title_encode,pdf_keywords_encode[i:i+1,:])\n",
    "    one_vector_list.append(one_vector.reshape(-1))\n",
    "np.array(one_vector_list).shape     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二层检索\n",
    "# 对pdf摘要进行编码处理\n",
    "pdf_subject_encode = model.encode(pdf_subject)\n",
    "\n",
    "# 对prompt内容进行编码处理\n",
    "# 给出文件列表\n",
    "dir_list = [f'./t{i}.txt' for i in range(1,5)]\n",
    "content_list = []\n",
    "# 读取四种类型的prompt\n",
    "for dir in dir_list :\n",
    "    with open(dir , 'r' , encoding='utf8') as f :\n",
    "        content = f.read()\n",
    "    content_list.append(content)\n",
    "content_list = model.encode(np.array(content_list))\n",
    "\n",
    "two_vector_list = []\n",
    "for i in range(pdf_subject_encode.shape[0]) :\n",
    "    one_vector = cosine_similarity(title_encode,pdf_subject_encode[i:i+1,:])\n",
    "    one_vector_list.append(one_vector.reshape(-1))\n",
    "np.array(two_vector_list).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用llama-api推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n",
    "import pandas as pd\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"\n",
    "You are an expert in the field of anaerobic fermentation. You should answer in the following format based on a full understanding of the literature.\n",
    "first, Briefly summarize the answers.\n",
    "second, Answer the question in more detail and logically, explaining every detail of the question in more detail. The format is\n",
    "    (1) Sub-explanation\n",
    "    (2) Sub-explanation\n",
    "etc...\n",
    "Sub-explanation should not be limited to two, usually three or more;\n",
    "finally, Summarize all the above conclusions and further explain the problem.\n",
    "\n",
    "{context}\n",
    "\n",
    "{chat_history}\n",
    "The question you are about to answer is:\n",
    "{question}\n",
    "answer:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(input_variables=[\"chat_history\",\"context\",\"question\"], template=template)\n",
    "prompt.format(question = question , context = document , chat_history = [])\n",
    "\n",
    "pc = template.format(question = question , context = document , chat_history = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_kwarg = {\n",
    "    \"prompt\": pc,\n",
    "    \"top_p\":1,\n",
    "    # \"max_length\":8000,\n",
    "    \"temperature\":0.75,\n",
    "    \"repetition_penalty\":1,\n",
    "    \"max_new_tokens\":1000,\n",
    "}\n",
    "\n",
    "o1 = replicate.run(\n",
    "        \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "        # \"meta/llama-2-13b-chat:f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d\",\n",
    "        # \"meta/llama-2-7b:527827021d8756c7ab79fde0abbfaac885c37a3ed5fe23c7465093f0878d55ef\",\n",
    "        # input={\"prompt\": \"What effect does the addition of biochar have on the biogas production of anaerobic digestion of food waste? What should its optimal addition amount be?\"}\n",
    "        input=input_kwarg\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\".join(list(o1))\n",
    "import time\n",
    "while True:\n",
    "    time.sleep(2)\n",
    "    print(o1.__next__())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myproject1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
